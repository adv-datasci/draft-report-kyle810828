---
title: "Scraing Data from Indeed"
author: "Kyle"
date: "9/26/2017"
output: html_document
---

###Session

#### Function: entering job, and location (text), Output: session (url)
######## Output the session of job and city (within 25 miles) you input
```{r,warning=FALSE}
library(rvest)
library(xml2)
library(stringr)
library(plyr)
library(httr)
library(dplyr)
library(rebus)

#### Find "data scientist" jobs in that city within 25 miles

session_loc<-function(job_text,location_text){
  query = job_text
  loc = location_text
  ###Find 
  
  session <- html_session(paste0("https://www.indeed.com/advanced_search?q=data+scientist&l=",loc,
                                 "&radius=25&limit=50"))
  form <- html_form(session)[[1]]
  form <- set_values(form, as_and = query, l = loc)
  # Submit value
  session_f <- submit_form(session, form)
  return(session_f)
}
```


###Job count
#### Function: entering session (previous function's output), and location(text), output: jobcount in that city
######## Output the session of job and city (within 25 miles) you input
```{r}
jobcount<-function(session_l,location_text){
  # get the html file from search url
  page<- read_html(session_l)
  # get the total job count 
  job_count <- unlist(strsplit(page %>% 
                                 html_node("#searchCount") %>%
                                 html_text(), split = ' ')) 
  job_count <- as.numeric(str_replace_all(job_count[length(job_count)],',',''))
  return(c(location_text,job_count))
}

```


###Basic Informaiton (contian title urls)
#### Funciton: entering job count (previous function's output), in order to scrape all data
######## Output title, company, location, titil urls


```{r}

basic_information<-function(jobcount){
  
  location_str=jobcount[1]
  jb=ceiling(as.numeric(jobcount[2])/50)
  BasicInfo<-matrix(NA,ncol = 4) 
  
  session <- paste("https://www.indeed.com/jobs?q=data+scientist&l=",location_str,"&radius=25&start="
                   ,seq(0,jb)*50,sep = "")
  
  for(i in 1:length(session)){
  
  joblink<- session[i] %>% read_html() %>% 
    html_nodes(xpath='//*[contains(concat( " ", @class, " " ), concat( " ", "result", " " ))]') %>% 
    html_node("h2 a")%>% html_attr("href")
  
  
  title<-session[i] %>% read_html() %>% 
    html_nodes(xpath='//*[contains(concat( " ", @class, " " ), concat( " ", "result", " " ))]') %>%
    html_node("a")%>% html_attr("title")
  
  company<-session[i] %>% read_html() %>% 
    html_nodes(xpath='//*[contains(concat( " ", @class, " " ), concat( " ", "result", " " ))]') %>%
    html_node("span a")%>% html_attr("href")
  
  location<-session[i] %>% read_html() %>% 
    html_nodes(xpath='//*[contains(concat( " ", @class, " " ), concat( " ", "result", " " ))]') %>%
    html_node("span.location") %>% html_text
  
  link_clean<-joblink[which(is.na(joblink)==FALSE)]
  title_clean<-title[which(is.na(joblink)==FALSE)]
  company_clean<-company[which(is.na(joblink)==FALSE)]
  location_clean<-location[which(is.na(joblink)==FALSE)]
  company_clean<-str_replace_all(company_clean, "[\r\n|\n|\t|\r|,|/|<|>|\\.]|cmp", '')
  
  CombineInfo<-cbind(title_clean,company_clean,location_clean,link_clean)
  BasicInfo<-rbind(BasicInfo,CombineInfo)
  Sys.sleep(00.1)
  }
  #Clean data, delete NA and which does not have company informaiton
  BasicInfo<-BasicInfo[complete.cases(BasicInfo)&BasicInfo[,"company_clean"]!="#",]
  return(as.data.frame(BasicInfo))
}
```

### Final output, Clean data set
#### Funciton: entering BasicInfo (matrix that created by funciton: basic_information)
######## Also can change required skillset in this chunck of code

```{r}

##Function need to clean text
clean.text <- function(text)
  {
    str_replace_all(text, regex('\r\n|\n|\t|\r|,|/|<|>|\\.'), ' ')
}


### Get job descreption (Skills that company required)
  
FinalData<-function(Infomatix){
  
  link_clean<-Infomatix$link_clean
  job.urls<-paste0("http://indeed.com",link_clean)
  
  ### Skill set I want to scrape 
  skillset <- c('Python','\\bSQL','\\bR\\b', 'SAS', 'Excel', 'C++', 'statistics', 'Java', 'Tableau')
  
  ###Scraping skill by going the title's urls
  skill<-c()
  
  
  
  tryCatch({
    
    for(i in 1:length(link_clean)){
    job.url <- paste0("http://indeed.com",link_clean[i])
    Sys.sleep(00.1)
    
    html <- read_html(job.url)
    text <- html_text(html)
    text <- clean.text(text)
    
    goodid<-which(str_detect(text, skillset)==TRUE)
    des<-paste0(unlist(skillset[goodid]),collapse =" ",sep=".")
    skill[i]<-gsub("\\b", "", des, fixed=TRUE)
    Sys.sleep(00.1)
  }
  
}, error=function(e){cat("ERROR :",conditionMessage(e), "\n")})
  
  
  
  
    FinalData<-cbind(title=as.character(Infomatix$title_clean),
                     company=as.character(Infomatix$company_clean),
                     location=as.character(Infomatix$location_clean),skills=skill)
  
  return(as.data.frame(FinalData))
}
```


```{r, echo=FALSE}

#All data science
All<-session_loc("data scientist","")
All_jobcount<-jobcount(All,"")

```

```{r}

##IL
#IL_info<-basic_information(IL_jobcount)
#IL_Data<-FinalData(IL_info)
##NY
#NY_info<-basic_information(NY_jobcount)
#NY_Data<-FinalData(NY_info)


#All data science
All_info<-basic_information(All_jobcount)
write.csv(All_info,"datascientist_indeed_info0930.csv")

d<-read.csv("datascientist.csv")
DataScientist<-FinalData(d)

####Clean data separate city and state
city<-c()
state<-c()

for(i in 1:nrow(DataScientist) ){
  city[i]<-strsplit(as.character(DataScientist$location), ",")[[i]][1]
  state[i]<-substr(strsplit(as.character(DataScientist$location), ",")[[i]][2],0,3)
}

DataScientist_f<-cbind(Title=as.character(DataScientist$title),
                     Company=as.character(DataScientist$company),
                     City=city,State=state,Skills=as.character(DataScientist$skills))

write.csv(DataScientist_f,"Indeed_datascientist.csv")

```